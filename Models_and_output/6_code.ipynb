{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.figure_factory as ff\n",
    "from IPython.display import HTML, display, SVG\n",
    "from IPython.core import display as ICD\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "\n",
    "import math\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding\n",
    "from keras.regularizers import L1L2\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn import neighbors\n",
    "from sklearn import linear_model\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator_shuffle(X_data, y_data, batch_size):\n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    index = np.arange(np.shape(y_data)[0])\n",
    "    np.random.shuffle(index)\n",
    "    while 1:\n",
    "        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X_data[index_batch,:]\n",
    "        y_batch = y_data[index_batch]\n",
    "        counter += 1\n",
    "        yield X_batch,y_batch\n",
    "        if (counter > number_of_batches):\n",
    "            np.random.shuffle(index)\n",
    "            counter=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_keras_metric(method):\n",
    "    import functools\n",
    "    from keras import backend as K\n",
    "    import tensorflow as tf\n",
    "    @functools.wraps(method)\n",
    "    def wrapper(self, args, **kwargs):\n",
    "        \"\"\" Wrapper for turning tensorflow metrics into keras metrics \"\"\"\n",
    "        value, update_op = method(self, args, **kwargs)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([update_op]):\n",
    "            value = tf.identity(value)\n",
    "        return value\n",
    "    return wrapper\n",
    "\n",
    "@as_keras_metric\n",
    "def auc_pr(y_true, y_pred, curve='PR'):\n",
    "    return tf.metrics.auc(y_true, y_pred, curve=curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 13\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_FOLDER = 'dataset/'\n",
    "OUTPUT_FOLDER = 'Models_and_output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(SOURCE_FOLDER + 'train.csv', encoding='ISO-8859-1')\n",
    "test_data = pd.read_csv(SOURCE_FOLDER + 'test.csv', encoding='utf8')\n",
    "test_data = test_data[test_data.columns[1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[train_data['TRANS_CONV_TEXT'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9b68741710>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "le.fit(train_data.Source)\n",
    "train_source = le.transform(train_data.Source)\n",
    "\n",
    "train_data.Patient_Tag.value_counts().plot('bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data['TRANS_CONV_TEXT']\n",
    "Y_train = train_data['Patient_Tag']\n",
    "Y_train = to_categorical(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X_train, stemmer, lemma):\n",
    "    preprocessed_data = []\n",
    "    trans = str.maketrans('/(){}', ' ' * 5)\n",
    "    trans_punc = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    for text in X_train:\n",
    "        text = text.lower().translate(trans)\n",
    "        text = text.translate(trans_punc)\n",
    "        text = [lemma.lemmatize(word) for word in text.split()]\n",
    "        preprocessed_data.append(' '.join(text))\n",
    "\n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = preprocess_data(X_train, stemmer, lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i went through a sleep study ahi severe at 95 and titration test in late 2011 but wa never able to sleep effectively i own the system one series 50 650 bipap pro that i wa prescribed and an opus 360 nasal pillow assembly both are essentially unused im about 20 pound lighter than i wa back then and my sleeping seems to be a little bit better now than it wa back then whats the best way of trying to address my sleep apnea again is it ok to just try my existing machine and setting and adjust using sleepyhead do i need to go back to the doc and or do another sleep study are there any particularly good mask since 2011 that might be better than what i have ive spent the last three week dealing with a father with congestive heart failure and id like to try and do what i can to avoid that'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(preprocessed_data)\n",
    "encoded_lines = t.texts_to_sequences(preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_words = len(t.word_index) + 1\n",
    "max_length_of_input = 1000\n",
    "embedding_vector_length = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sequence.pad_sequences(encoded_lines, max_length_of_input)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, Y_train, test_size=0.33, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(number_of_words, embedding_vector_length, input_length = max_length_of_input))\n",
    "# Uncomment the below line if you want to use Dropout\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(2, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "Epoch 1/6\n",
      "25/24 [===============================] - 21s 860ms/step - loss: 0.0020 - auc_pr: 0.9599 - acc: 1.0000 - val_loss: 0.8783 - val_auc_pr: 0.9856 - val_acc: 0.8796\n",
      "Epoch 2/6\n",
      "25/24 [===============================] - 20s 806ms/step - loss: 7.2443e-05 - auc_pr: 0.9811 - acc: 1.0000 - val_loss: 0.7238 - val_auc_pr: 0.9814 - val_acc: 0.9005\n",
      "Epoch 3/6\n",
      "25/24 [===============================] - 21s 852ms/step - loss: 3.0398e-05 - auc_pr: 0.9809 - acc: 1.0000 - val_loss: 0.7544 - val_auc_pr: 0.9807 - val_acc: 0.8979\n",
      "Epoch 4/6\n",
      "25/24 [===============================] - 20s 797ms/step - loss: 2.2041e-05 - auc_pr: 0.9805 - acc: 1.0000 - val_loss: 0.8168 - val_auc_pr: 0.9804 - val_acc: 0.8770\n",
      "Epoch 5/6\n",
      "25/24 [===============================] - 20s 791ms/step - loss: 1.9127e-05 - auc_pr: 0.9802 - acc: 1.0000 - val_loss: 0.8542 - val_auc_pr: 0.9801 - val_acc: 0.8743\n",
      "Epoch 6/6\n",
      "25/24 [===============================] - 20s 792ms/step - loss: 1.3188e-05 - auc_pr: 0.9799 - acc: 1.0000 - val_loss: 0.8538 - val_auc_pr: 0.9799 - val_acc: 0.8796\n",
      "382/382 [==============================] - 2s 5ms/step\n",
      "\n",
      "\n",
      "[0.8538091270087277, 0.9776763326210501, 0.8795811502721297]\n"
     ]
    }
   ],
   "source": [
    "precision = as_keras_metric(tf.metrics.precision)\n",
    "recall = as_keras_metric(tf.metrics.recall)\n",
    "\n",
    "model.compile(optimizer='Adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=[auc_pr, 'accuracy'])\n",
    "\n",
    "mc = keras.callbacks.ModelCheckpoint('weights{epoch:08d}.h5', \n",
    "                                     save_weights_only=True, period=5)\n",
    "\n",
    "model.fit_generator(generator=batch_generator_shuffle(X_train, y_train, 32),\n",
    "                      epochs=6, validation_data=(X_valid, y_valid),\n",
    "                      steps_per_epoch=X_train.shape[0] / 32, callbacks=[mc])\n",
    "\n",
    "scores = model.evaluate(X_valid, y_valid)\n",
    "print ('\\n')\n",
    "print (scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - 7: val_loss: 0.3521 - val_auc_pr: 0.9775 - val_acc: 0.8922\n",
    "2 - 200: val_loss: 0.6366 - val_auc_pr: 0.9851 - val_acc: 0.8889\n",
    "3 - val_loss: 1.3443 - val_auc_pr: 0.9714 - val_acc: 0.8246\n",
    "\n",
    "[0.626494295846403, 0.983139782170065, 0.8856209134743884]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data['TRANS_CONV_TEXT']\n",
    "X_test = preprocess_data(X_test, stemmer, lemma)\n",
    "X_test = t.texts_to_sequences(X_test)\n",
    "X_test = sequence.pad_sequences(X_test, max_length_of_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ..., 3934,  743,  982],\n",
       "       [   0,    0,    0, ...,  929,  395,  176],\n",
       "       [   0,    0,    0, ..., 7374, 7375, 4014],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  114,  296,   35],\n",
       "       [   0,    0,    0, ...,   68,   12,    7],\n",
       "       [   0,    0,    0, ...,   26,  172,  668]], dtype=int32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.predict(X_test, 32)\n",
    "\n",
    "patient_tag = np.argmax(output, axis=1)\n",
    "index = list(range(1, len(output)+1))\n",
    "test_data_df = pd.DataFrame({'Index': index,'Patient_Tag': patient_tag}).set_index('Index')\n",
    "test_data_df.to_csv('output.csv', columns=['Patient_Tag'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    470\n",
       "1    101\n",
       "Name: Patient_Tag, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_df.Patient_Tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_json = model.to_json()\n",
    "# with open(OUTPUT_FOLDER + \"3_model.json\", \"w\") as json_file:\n",
    "#     json_file.write(model_json)\n",
    "\n",
    "# model.save_weights(OUTPUT_FOLDER + \"3_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    470\n",
       "1    101\n",
       "Name: Patient_Tag, dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1 = pd.read_csv('Models_and_output/3_output_85_11384.csv')\n",
    "out1.Patient_Tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    476\n",
       "1     95\n",
       "Name: Patient_Tag, dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2 = pd.read_csv('/home/abhishek/Downloads/IdentifyOnlinePatientConversations-master/output.csv')\n",
    "out2.Patient_Tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
