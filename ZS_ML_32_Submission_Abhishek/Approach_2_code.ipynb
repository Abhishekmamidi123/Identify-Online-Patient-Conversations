{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Identify online patient conversations </center>\n",
    "## Approach: Embedding + LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML, display, SVG\n",
    "from IPython.core import display as ICD\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.regularizers import L1L2\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include ROC AUC as metric in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_keras_metric(method):\n",
    "    import functools\n",
    "    from keras import backend as K\n",
    "    import tensorflow as tf\n",
    "    @functools.wraps(method)\n",
    "    def wrapper(self, args, **kwargs):\n",
    "        \"\"\" Wrapper for turning tensorflow metrics into keras metrics \"\"\"\n",
    "        value, update_op = method(self, args, **kwargs)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([update_op]):\n",
    "            value = tf.identity(value)\n",
    "        return value\n",
    "    return wrapper\n",
    "\n",
    "@as_keras_metric\n",
    "def auc_pr(y_true, y_pred, curve='PR'):\n",
    "    return tf.metrics.auc(y_true, y_pred, curve=curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle data to avoid bias while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator_shuffle(X_data, y_data, batch_size):\n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    index = np.arange(np.shape(y_data)[0])\n",
    "    np.random.shuffle(index)\n",
    "    while 1:\n",
    "        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X_data[index_batch,:]\n",
    "        y_batch = y_data[index_batch]\n",
    "        counter += 1\n",
    "        yield X_batch,y_batch\n",
    "        if (counter > number_of_batches):\n",
    "            np.random.shuffle(index)\n",
    "            counter=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 8\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_FOLDER = 'dataset/'\n",
    "OUTPUT_FOLDER = 'Models_and_output/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(SOURCE_FOLDER + 'train.csv', encoding='ISO-8859-1')\n",
    "test_data = pd.read_csv(SOURCE_FOLDER + 'test.csv', encoding='utf8')\n",
    "test_data = test_data[test_data.columns[1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Host</th>\n",
       "      <th>Link</th>\n",
       "      <th>Date(ET)</th>\n",
       "      <th>Time(ET)</th>\n",
       "      <th>time(GMT)</th>\n",
       "      <th>Title</th>\n",
       "      <th>TRANS_CONV_TEXT</th>\n",
       "      <th>Patient_Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FORUMS</td>\n",
       "      <td>cafepharma.com</td>\n",
       "      <td>http://cafepharma.com/boards/threads/epstein.5...</td>\n",
       "      <td>6/15/2016</td>\n",
       "      <td>13:58:00</td>\n",
       "      <td>6/15/2016 23:28</td>\n",
       "      <td>Epstein</td>\n",
       "      <td>I don't disagree with you in principle. I'm ju...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FORUMS</td>\n",
       "      <td>www.patient.co.uk</td>\n",
       "      <td>http://www.patient.co.uk/forums/discuss/enlarg...</td>\n",
       "      <td>5/7/2016</td>\n",
       "      <td>0.820833333</td>\n",
       "      <td>42498.21667</td>\n",
       "      <td>Enlarged Heart.Thread Enlarged Heart</td>\n",
       "      <td>I am always dizzy I get dizzy standing up so I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BLOG</td>\n",
       "      <td>http://abcnewsradioonline.com/entertainment-news</td>\n",
       "      <td>http://abcnewsradioonline.com/entertainment-ne...</td>\n",
       "      <td>4/14/2016</td>\n",
       "      <td>15:00:38</td>\n",
       "      <td>4/15/2016 0:30</td>\n",
       "      <td>Queen Latifah Joins American Heart Association...</td>\n",
       "      <td>Axelle/Bauer-Griffin/FilmMagic(NEW YORK) -- Qu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FORUMS</td>\n",
       "      <td>www.cancer-forums.net</td>\n",
       "      <td>http://www.cancer-forums.net/viewtopic.php?f=1...</td>\n",
       "      <td>6/18/2016</td>\n",
       "      <td>20:46:00</td>\n",
       "      <td>6/19/2016 6:16</td>\n",
       "      <td>Bulaemia</td>\n",
       "      <td>I am 17 and I have been throwing up for about ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FORUMS</td>\n",
       "      <td>www.diyaudio.com</td>\n",
       "      <td>http://www.diyaudio.com/forums/lounge/292252-d...</td>\n",
       "      <td>6/15/2016</td>\n",
       "      <td>3:26:00</td>\n",
       "      <td>6/15/2016 12:56</td>\n",
       "      <td>DIY Silver interconnects and RCAs???</td>\n",
       "      <td>Quote: Originally Posted by Boyan Silyavski Wa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Source                                              Host  \\\n",
       "0  FORUMS                                    cafepharma.com   \n",
       "1  FORUMS                                 www.patient.co.uk   \n",
       "2    BLOG  http://abcnewsradioonline.com/entertainment-news   \n",
       "3  FORUMS                             www.cancer-forums.net   \n",
       "4  FORUMS                                  www.diyaudio.com   \n",
       "\n",
       "                                                Link   Date(ET)     Time(ET)  \\\n",
       "0  http://cafepharma.com/boards/threads/epstein.5...  6/15/2016     13:58:00   \n",
       "1  http://www.patient.co.uk/forums/discuss/enlarg...   5/7/2016  0.820833333   \n",
       "2  http://abcnewsradioonline.com/entertainment-ne...  4/14/2016     15:00:38   \n",
       "3  http://www.cancer-forums.net/viewtopic.php?f=1...  6/18/2016     20:46:00   \n",
       "4  http://www.diyaudio.com/forums/lounge/292252-d...  6/15/2016      3:26:00   \n",
       "\n",
       "         time(GMT)                                              Title  \\\n",
       "0  6/15/2016 23:28                                            Epstein   \n",
       "1      42498.21667               Enlarged Heart.Thread Enlarged Heart   \n",
       "2   4/15/2016 0:30  Queen Latifah Joins American Heart Association...   \n",
       "3   6/19/2016 6:16                                           Bulaemia   \n",
       "4  6/15/2016 12:56               DIY Silver interconnects and RCAs???   \n",
       "\n",
       "                                     TRANS_CONV_TEXT  Patient_Tag  \n",
       "0  I don't disagree with you in principle. I'm ju...            0  \n",
       "1  I am always dizzy I get dizzy standing up so I...            1  \n",
       "2  Axelle/Bauer-Griffin/FilmMagic(NEW YORK) -- Qu...            0  \n",
       "3  I am 17 and I have been throwing up for about ...            1  \n",
       "4  Quote: Originally Posted by Boyan Silyavski Wa...            0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider only 'TRANS_CONV_TEXT' column for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[train_data['TRANS_CONV_TEXT'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f73769dd048>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data.Patient_Tag.value_counts().plot('bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data['TRANS_CONV_TEXT']\n",
    "Y_train = train_data['Patient_Tag']\n",
    "Y_train = to_categorical(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text\n",
    "- Remove punctuations from data.\n",
    "- Apply lemmatization on words. Convert each word to its lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X_train, stemmer, lemma):\n",
    "    preprocessed_data = []\n",
    "    trans = str.maketrans('/(){}', ' ' * 5)\n",
    "    trans_punc = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    for text in X_train:\n",
    "        text = text.lower().translate(trans)\n",
    "        text = text.translate(trans_punc)\n",
    "        text = [lemma.lemmatize(word) for word in text.split()]\n",
    "        preprocessed_data.append(' '.join(text))\n",
    "\n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = preprocess_data(X_train, stemmer, lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i went through a sleep study ahi severe at 95 and titration test in late 2011 but wa never able to sleep effectively i own the system one series 50 650 bipap pro that i wa prescribed and an opus 360 nasal pillow assembly both are essentially unused im about 20 pound lighter than i wa back then and my sleeping seems to be a little bit better now than it wa back then whats the best way of trying to address my sleep apnea again is it ok to just try my existing machine and setting and adjust using sleepyhead do i need to go back to the doc and or do another sleep study are there any particularly good mask since 2011 that might be better than what i have ive spent the last three week dealing with a father with congestive heart failure and id like to try and do what i can to avoid that'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize text\n",
    "- Tokenize text and convert them into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(preprocessed_data)\n",
    "encoded_lines = t.texts_to_sequences(preprocessed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_words = len(t.word_index) + 1\n",
    "max_length_of_input = 1000\n",
    "embedding_vector_length = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad sequences to same length and split data to form validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sequence.pad_sequences(encoded_lines, max_length_of_input)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, Y_train, test_size=0.33, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(number_of_words, embedding_vector_length, input_length = max_length_of_input))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(2, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "Epoch 1/5\n",
      "25/24 [===============================] - 40s 2s/step - loss: 0.5824 - auc_pr: 0.7130 - acc: 0.7795 - val_loss: 0.5073 - val_auc_pr: 0.7588 - val_acc: 0.8063\n",
      "Epoch 2/5\n",
      "25/24 [===============================] - 38s 2s/step - loss: 0.4682 - auc_pr: 0.7880 - acc: 0.7870 - val_loss: 0.4163 - val_auc_pr: 0.8113 - val_acc: 0.8063\n",
      "Epoch 3/5\n",
      "25/24 [===============================] - 38s 2s/step - loss: 0.2288 - auc_pr: 0.8532 - acc: 0.9024 - val_loss: 0.2552 - val_auc_pr: 0.8879 - val_acc: 0.8848\n",
      "Epoch 4/5\n",
      "25/24 [===============================] - 43s 2s/step - loss: 0.0755 - auc_pr: 0.9109 - acc: 0.9837 - val_loss: 0.2670 - val_auc_pr: 0.9274 - val_acc: 0.9084\n",
      "Epoch 5/5\n",
      "25/24 [===============================] - 39s 2s/step - loss: 0.0246 - auc_pr: 0.9385 - acc: 0.9962 - val_loss: 0.3576 - val_auc_pr: 0.9464 - val_acc: 0.8953\n",
      "382/382 [==============================] - 3s 8ms/step\n",
      "\n",
      "\n",
      "[0.3576499744859665, 0.9451076506944227, 0.8952879593634481]\n"
     ]
    }
   ],
   "source": [
    "precision = as_keras_metric(tf.metrics.precision)\n",
    "recall = as_keras_metric(tf.metrics.recall)\n",
    "\n",
    "model.compile(optimizer='Adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=[auc_pr, 'accuracy'])\n",
    "\n",
    "mc = keras.callbacks.ModelCheckpoint('weights{epoch:02d}.h5', \n",
    "                                     save_weights_only=True, period=5)\n",
    "\n",
    "# Try 40 epochs\n",
    "model.fit_generator(generator=batch_generator_shuffle(X_train, y_train, 32),\n",
    "                      epochs=5, validation_data=(X_valid, y_valid),\n",
    "                      steps_per_epoch=X_train.shape[0] / 32, callbacks=[mc])\n",
    "\n",
    "scores = model.evaluate(X_valid, y_valid)\n",
    "print ('\\n')\n",
    "print (scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess test data and convert into vector form "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data['TRANS_CONV_TEXT']\n",
    "X_test = preprocess_data(X_test, stemmer, lemma)\n",
    "X_test = t.texts_to_sequences(X_test)\n",
    "X_test = sequence.pad_sequences(X_test, max_length_of_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(571, 1000)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict output and save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.predict(X_test, 32)\n",
    "\n",
    "patient_tag = np.argmax(output, axis=1)\n",
    "index = list(range(1, len(output)+1))\n",
    "test_data_df = pd.DataFrame({'Index': index,'Patient_Tag': patient_tag}).set_index('Index')\n",
    "test_data_df.Patient_Tag = test_data_df.Patient_Tag.astype('int')\n",
    "test_data_df.to_csv('Approach_2_output.csv', columns=['Patient_Tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
